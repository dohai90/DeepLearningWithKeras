{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "path = keras.utils.get_file('nietzsche.txt', \n",
    "                            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "\n",
    "text = open(path, 'r').read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences 200278\n",
      "Number of unique characters 57\n",
      "Vectorizing...\n"
     ]
    }
   ],
   "source": [
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i:i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sentences', len(sentences))\n",
    "chars = sorted(list(set(text)))\n",
    "print('Number of unique characters', len(chars))\n",
    "char_indices = dict((char, i) for i, char in enumerate(chars))\n",
    "\n",
    "print('Vectorizing...')\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds a model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(keras.layers.Dense(len(chars), activation=keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.01),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160222 samples, validate on 40056 samples\n",
      "Epoch 1/60\n",
      "160222/160222 [==============================] - 166s 1ms/step - loss: 2.0651 - categorical_accuracy: 0.4003 - val_loss: 1.8019 - val_categorical_accuracy: 0.4682\n",
      "Epoch 2/60\n",
      "160222/160222 [==============================] - 162s 1ms/step - loss: 1.6772 - categorical_accuracy: 0.5040 - val_loss: 1.6752 - val_categorical_accuracy: 0.5013\n",
      "Epoch 3/60\n",
      "160222/160222 [==============================] - 163s 1ms/step - loss: 1.5696 - categorical_accuracy: 0.5332 - val_loss: 1.6141 - val_categorical_accuracy: 0.5222\n",
      "Epoch 4/60\n",
      "160222/160222 [==============================] - 161s 1ms/step - loss: 1.5140 - categorical_accuracy: 0.5493 - val_loss: 1.6050 - val_categorical_accuracy: 0.5270\n",
      "Epoch 5/60\n",
      "160222/160222 [==============================] - 162s 1ms/step - loss: 1.4768 - categorical_accuracy: 0.5587 - val_loss: 1.5866 - val_categorical_accuracy: 0.5304\n",
      "Epoch 6/60\n",
      "160222/160222 [==============================] - 162s 1ms/step - loss: 1.4510 - categorical_accuracy: 0.5637 - val_loss: 1.5843 - val_categorical_accuracy: 0.5369\n",
      "Epoch 7/60\n",
      "160222/160222 [==============================] - 159s 993us/step - loss: 1.4315 - categorical_accuracy: 0.5706 - val_loss: 1.5880 - val_categorical_accuracy: 0.5352\n",
      "Epoch 8/60\n",
      "160222/160222 [==============================] - 160s 998us/step - loss: 1.4152 - categorical_accuracy: 0.5749 - val_loss: 1.5849 - val_categorical_accuracy: 0.5369\n",
      "Epoch 9/60\n",
      "160222/160222 [==============================] - 161s 1ms/step - loss: 1.4017 - categorical_accuracy: 0.5784 - val_loss: 1.5769 - val_categorical_accuracy: 0.5344\n",
      "Epoch 10/60\n",
      "160222/160222 [==============================] - 156s 971us/step - loss: 1.3911 - categorical_accuracy: 0.5810 - val_loss: 1.5860 - val_categorical_accuracy: 0.5353\n",
      "Epoch 11/60\n",
      "160222/160222 [==============================] - 160s 1ms/step - loss: 1.3821 - categorical_accuracy: 0.5823 - val_loss: 1.5778 - val_categorical_accuracy: 0.5416\n",
      "Epoch 12/60\n",
      "160222/160222 [==============================] - 161s 1ms/step - loss: 1.3721 - categorical_accuracy: 0.5857 - val_loss: 1.5867 - val_categorical_accuracy: 0.5404\n",
      "Epoch 13/60\n",
      "160222/160222 [==============================] - 157s 981us/step - loss: 1.3631 - categorical_accuracy: 0.5878 - val_loss: 1.5710 - val_categorical_accuracy: 0.5422\n",
      "Epoch 14/60\n",
      "160222/160222 [==============================] - 158s 985us/step - loss: 1.3548 - categorical_accuracy: 0.5889 - val_loss: 1.5827 - val_categorical_accuracy: 0.5399\n",
      "Epoch 15/60\n",
      "160222/160222 [==============================] - 157s 979us/step - loss: 1.3500 - categorical_accuracy: 0.5905 - val_loss: 1.5924 - val_categorical_accuracy: 0.5412\n",
      "Epoch 16/60\n",
      "160222/160222 [==============================] - 155s 969us/step - loss: 1.3430 - categorical_accuracy: 0.5932 - val_loss: 1.5846 - val_categorical_accuracy: 0.5429\n",
      "Epoch 17/60\n",
      "160222/160222 [==============================] - 157s 979us/step - loss: 1.3376 - categorical_accuracy: 0.5942 - val_loss: 1.5916 - val_categorical_accuracy: 0.5407\n",
      "Epoch 18/60\n",
      "160222/160222 [==============================] - 157s 981us/step - loss: 1.3325 - categorical_accuracy: 0.5956 - val_loss: 1.5844 - val_categorical_accuracy: 0.5431\n",
      "Epoch 19/60\n",
      "160222/160222 [==============================] - 154s 963us/step - loss: 1.3249 - categorical_accuracy: 0.5978 - val_loss: 1.6092 - val_categorical_accuracy: 0.5397\n",
      "Epoch 20/60\n",
      "160222/160222 [==============================] - 156s 976us/step - loss: 1.3229 - categorical_accuracy: 0.5984 - val_loss: 1.5900 - val_categorical_accuracy: 0.5394\n",
      "Epoch 21/60\n",
      "160222/160222 [==============================] - 156s 975us/step - loss: 1.3182 - categorical_accuracy: 0.5986 - val_loss: 1.6130 - val_categorical_accuracy: 0.5414\n",
      "Epoch 22/60\n",
      "160222/160222 [==============================] - 156s 975us/step - loss: 1.3154 - categorical_accuracy: 0.5996 - val_loss: 1.5980 - val_categorical_accuracy: 0.5416\n",
      "Epoch 23/60\n",
      "160222/160222 [==============================] - 155s 969us/step - loss: 1.3112 - categorical_accuracy: 0.6009 - val_loss: 1.5831 - val_categorical_accuracy: 0.5437\n",
      "Epoch 24/60\n",
      "160222/160222 [==============================] - 157s 979us/step - loss: 1.3073 - categorical_accuracy: 0.6020 - val_loss: 1.6045 - val_categorical_accuracy: 0.5387\n",
      "Epoch 25/60\n",
      "160222/160222 [==============================] - 156s 973us/step - loss: 1.3029 - categorical_accuracy: 0.6024 - val_loss: 1.6046 - val_categorical_accuracy: 0.5374\n",
      "Epoch 26/60\n",
      "160222/160222 [==============================] - 158s 983us/step - loss: 1.3005 - categorical_accuracy: 0.6030 - val_loss: 1.6017 - val_categorical_accuracy: 0.5446\n",
      "Epoch 27/60\n",
      "160222/160222 [==============================] - 157s 977us/step - loss: 1.2961 - categorical_accuracy: 0.6044 - val_loss: 1.6035 - val_categorical_accuracy: 0.5424\n",
      "Epoch 28/60\n",
      "160222/160222 [==============================] - 155s 966us/step - loss: 1.2923 - categorical_accuracy: 0.6053 - val_loss: 1.5937 - val_categorical_accuracy: 0.5435\n",
      "Epoch 29/60\n",
      "160222/160222 [==============================] - 155s 968us/step - loss: 1.2903 - categorical_accuracy: 0.6065 - val_loss: 1.6062 - val_categorical_accuracy: 0.5444\n",
      "Epoch 30/60\n",
      "160222/160222 [==============================] - 156s 977us/step - loss: 1.2894 - categorical_accuracy: 0.6056 - val_loss: 1.6283 - val_categorical_accuracy: 0.5425\n",
      "Epoch 31/60\n",
      "160222/160222 [==============================] - 157s 980us/step - loss: 1.2857 - categorical_accuracy: 0.6067 - val_loss: 1.6144 - val_categorical_accuracy: 0.5409\n"
     ]
    }
   ],
   "source": [
    "fname = '/content/dohai90/workspace/keras/checkpoints/text_gen_ckpt_{epoch:02d}.h5'\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy',\n",
    "                                                patience=5),\n",
    "                  keras.callbacks.ModelCheckpoint(filepath=fname,\n",
    "                                                 monitor='loss',\n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min')]\n",
    "\n",
    "history = model.fit(x, y, epochs=60, batch_size=128, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/content/dohai90/workspace/keras/checkpoints/text_gen_ckpt_29.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample the next character given the model's predictions\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(np.float64)\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with seed: f. this is far from being the general human opinion. it is\n",
      "n\n",
      "\n",
      "Temperature: 0.2\n",
      "f. this is far from being the general human opinion. it is\n",
      "not the strong the spirit and spirity of the spirit and subtle of the sign and the spirit and subtle of the similar the morality and seems to the strong conscience of the spirit to the contrary to the continuant that the sense of the spirit to the spirit and conscience of the spirit and subtle of the sentence and still be stronger to the strengthing of the sentence and subtle of the spirit and sens\n",
      "Temperature: 0.5\n",
      "f. this is far from being the general human opinion. it is\n",
      "not the greatest of the words of the hard, it is always of the philosophers. the passion of the tendence of the more perhaps of the super--do not at the conceal is to make the contrary has been are so much as \"such a sense with\n",
      "the god of the world even that means, the greatest and surentary really any one wish and say, as the soul, the clous gangervation, the greatest and soul, the great any open \n",
      "Temperature: 1.0\n",
      "f. this is far from being the general human opinion. it is\n",
      "nothing in danger frumal good, even with summent\n",
      "of their continual dere nowedwwict as the philosopher are sentiment\n",
      "the cause, to schoor which has\n",
      "defined by what\n",
      "deep the quirien our first above it of his god our way as the more slave) an hill; with his abstrainity for the whendien thinker desirablesh say, the words hostem forment under advance of moral\n",
      "men, the\n",
      "wreader must never\n",
      "conscience,\n",
      "sua\n",
      "Temperature: 1.2\n",
      "f. this is far from being the general human opinion. it is\n",
      "notires, for which taking he is\n",
      "in fearless\" and too\n",
      "philoso"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pher suffering only sil and weckes a\n",
      "-int a all-so thought, prigiless, by life-frem which--this plato since, within semico its the fact live-croibles of life. their over life: it\n",
      "must not eurority.g-outy onour conslato its hyped, at lisang for saint ntousina feet whom the spirit, enduescarderdorwly,\n",
      "what learne, nor peollely mad himself ar"
     ]
    }
   ],
   "source": [
    "# generates text\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "seed_text = text[start_index:start_index + maxlen]\n",
    "print('Generating with seed:', seed_text)\n",
    "\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print('\\nTemperature:', temperature)\n",
    "    generated_text = seed_text\n",
    "    sys.stdout.write(generated_text)\n",
    "    for i in range(400):\n",
    "        # one hot encodes the generated text so far\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.\n",
    "            \n",
    "        preds = model.predict(sampled)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "        \n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "        sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
